Perceptron <- function(patterns, targets) {
#random initialisation of weights using the uniform [0,1] distribution
#remember that the size of the weight vector is the number of diamensions in patterns
w = as.vector(c(runif(ncol(patterns))))
#set learning rate eta
eta = 0.25
#get number of data points
n = nrow(patterns)
#create a vector to store predictions
pred =  matrix(nrow = n, ncol = 1)
K = 50
# We are now going to cycle through all the data points using a loop:
# 1) checking if each data point is misclassified
# 2) if it is(misclassified) - we update the weights using the update rule in the notes
# Repeat the cycling process until no data point is misclassified or after 100 cycles/epochs
for (i in 1:K){ # repeat a maximum of K times
for (j in 1:n){
y_j = t(w)%*%patterns[j,] #multiplication for y_in
pred[j] = sign(y_j) #basically thresholding
if ( y_j * targets[j]< 0) {
w = w + eta * targets[j] * patterns[j , ]
}
}
}
accuracy  = sum(pred==targets)/n*100
print(paste(' % Train Accuracy : ', accuracy))
return (w)
}
setwd("~/Google Drive/Statistical Machine Learning Course/Labs")
A=rbind((1,0,1,0,0),c(1,1,1,1,0),c(0,1,0,1,0),c(1,1,1,0,1),c(0,0,1,0,1),c(1,1,0,1,1))
A=rbind(c(1,0,1,0,0),c(1,1,1,1,0),c(0,1,0,1,0),c(1,1,1,0,1),c(0,0,1,0,1),c(1,1,0,1,1))
View(A)
targets = as.vector(1,-1,1,-1,1,-1)
targets = as.vector(c(1,-1,1,-1,1,-1))
Perceptron(A,targets)
K = 2
source('~/.active-rstudio-document')
Perceptron(A,targets)
A=rbind(c(1,0,1,0,0),c(1,1,1,1,0),c(0,1,0,1,0),c(1,1,1,0,1),c(0,0,1,0,1),c(1,1,0,1,1))
A= as.matrix(A)
targets = as.vector(c(1,-1,1,-1,1,-1))
Perceptron <- function(patterns, targets) {
#random initialisation of weights using the uniform [0,1] distribution
#remember that the size of the weight vector is the number of diamensions in patterns
w = as.vector(c(runif(ncol(patterns))))
#set learning rate eta
eta = 0.25
#get number of data points
n = nrow(patterns)
#create a vector to store predictions
pred =  matrix(nrow = n, ncol = 1)
K = 2
# We are now going to cycle through all the data points using a loop:
# 1) checking if each data point is misclassified
# 2) if it is(misclassified) - we update the weights using the update rule in the notes
# Repeat the cycling process until no data point is misclassified or after 100 cycles/epochs
for (i in 1:K){ # repeat a maximum of K times
for (j in 1:n){
y_j = t(w)%*%patterns[j,] #multiplication for y_in
pred[j] = sign(y_j) #basically thresholding
if ( y_j * targets[j]< 0) {
w = w + eta * targets[j] * patterns[j , ]
}
}
}
accuracy  = sum(pred==targets)/n*100
print(paste(' % Train Accuracy : ', accuracy))
return (w)
}
Perceptron(A,targets)
A=rbind(c(1,0,1,0,0),c(1,1,1,1,0),c(0,1,0,1,0),c(1,1,1,0,1),c(0,0,1,0,1),c(1,1,0,1,1))
A= as.matrix(A)
targets = as.vector(c(1,-1,1,-1,1,-1))
Perceptron <- function(patterns, targets) {
#random initialisation of weights using the uniform [0,1] distribution
#remember that the size of the weight vector is the number of diamensions in patterns
w = as.vector(c(runif(ncol(patterns))))
#set learning rate eta
eta = 0.25
#get number of data points
n = nrow(patterns)
#create a vector to store predictions
pred =  matrix(nrow = n, ncol = 1)
K = 2
# We are now going to cycle through all the data points using a loop:
# 1) checking if each data point is misclassified
# 2) if it is(misclassified) - we update the weights using the update rule in the notes
# Repeat the cycling process until no data point is misclassified or after 100 cycles/epochs
for (i in 1:K){ # repeat a maximum of K times
for (j in 1:n){
y_j = t(w)%*%patterns[j,] #multiplication for y_in
pred[j] = sign(y_j) #basically thresholding
if ( y_j * targets[j]< 0) {
w = w + eta * targets[j] * patterns[j , ]
}
}
}
accuracy  = sum(pred==targets)/n*100
print(paste(' % Train Accuracy : ', accuracy))
return (w)
}
Perceptron(A,targets)
source('~/Google Drive/Statistical Machine Learning Course/Labs/Exam.R')
source('~/Google Drive/Statistical Machine Learning Course/Labs/Exam.R')
source('~/Google Drive/Statistical Machine Learning Course/Labs/Exam.R')
source('~/Google Drive/Statistical Machine Learning Course/Labs/Exam.R')
View(A)
#random initialisation of weights using the uniform [0,1] distribution
#remember that the size of the weight vector is the number of diamensions in patterns
w = as.vector(c(0,0,0,0,0,0))
Perceptron(A,targets)
source('~/Google Drive/Statistical Machine Learning Course/Labs/Exam.R')
source('~/Google Drive/Statistical Machine Learning Course/Labs/Exam.R')
source('~/Google Drive/Statistical Machine Learning Course/Labs/Exam.R')
source('~/Google Drive/Statistical Machine Learning Course/Labs/Exam.R')
source('~/Google Drive/Statistical Machine Learning Course/Labs/Exam.R')
K = 2
source('~/Google Drive/Statistical Machine Learning Course/Labs/Exam.R')
